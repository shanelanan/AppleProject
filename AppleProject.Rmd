---
title: "Apple Case Study"
author: "Shane Lanan"
date: "January 18, 2019"
output:
  html_document: 
    fig_height: 6
    fig_width: 9
    theme: spacelab
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Extract, Transform, and Load

A Python 3.7 backend was developed to:  

* Pull the data from its sources
* Prepare for a load into Sqlite
* Commit records and report upload status


In a production environment:  

* Use an enterprise warehouse solution like Hadoop, Teradata, Oracle, SQL Server, MySQL, etc.
* Schedule the script in Unix via /etc/crontab


```{python etl}
#!/usr/bin/python3

import urllib.request
import pandas as pd
import io
import sqlite3
from datetime import datetime
from tqdm import tqdm


dataUrl = "http://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom.data"
labelUrl = "http://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom_labels.data"
vendorUrl = "./data/vendordata.json"

# column names for sqlite
newCols = {
    "datetime": "MFG_DATE",
    "mat vendor": "MAT_VENDOR",
    "part vendor": "PART_VENDOR",
    "sil vendor": "SIL_VENDOR",
    "adhs vendor": "ADHS_VENDOR",
    "sop vendor": "SOP_VENDOR",
}


def dfUpload(df, con, table, timeStamp=True, clearTable=False, debug=False):
    if timeStamp:
        df['INSERTED_ON'] = datetime.now()

    df = df.where(pd.notnull(df), None)  # convert NaN to None, for SQL Nulls
    # just to fix pd.NaT to insert as NULLS
    for col in df.columns:
        if df[col].dtype.kind == 'M':
            df[col] = df[col].astype(object).where(df[col].notnull(), None)
            df[col] = df[col].dt.strftime('%Y-%m-%d %h:%m:%s')

    sqlColumns = '(' + ','.join([col for col in df.columns]) + ')'
    sqlValues = '(' + ','.join([':' + str(x + 1) for x in list(range(len(df.columns)))]) + ')'
    sqlInsert = "INSERT INTO %s %s VALUES %s" % (table, sqlColumns, sqlValues)
    crsr = con.cursor()

    # uploading
    if clearTable:
        crsr.execute("DELETE FROM %s" % table)

    for row in tqdm(df.values.tolist(), desc="Uploading data", unit="row"):
        if debug:
            try:
                crsr.executemany(sqlInsert, [row])
            except:
                print(row)
                pass
        else:
            crsr.executemany(sqlInsert, [row])

    con.commit()
    crsr.close()


def main():

    # tried pd.read_html(), but no tables found?
    def PandasFromUrl(url):
        return pd.read_csv(io.BytesIO(urllib.request.urlopen(url).read()),
                           encoding="utf8", sep=" ", header=None)

    print("Fetching data from web and formatting...")
    data = PandasFromUrl(dataUrl)
    data.columns = ["F" + str(i) for i in range(len(data.columns))]  # prefix feature columns with "F"
    data['PASS_FAIL'] = PandasFromUrl(labelUrl)[0]
    vendors = pd.read_json(vendorUrl).sort_index()
    df = data.merge(vendors, left_index=True, right_index=True)
    df.rename(index=str, columns=newCols, inplace=True)
    df['ID'] = list(range(len(df)))

    print("Connecting to Sqlite...")
    con = sqlite3.connect("warehouse.db")
    print("Clearing table and inserting records...")
    dfUpload(df, con, "SAMPLE", clearTable=True)
    print("Disconnecting from Sqlite...")
    con.close()
    print("Done!")
    
    
if __name__ == '__main__':
    main()
```


## Prepare Environment

* Loading required libraries  
* Clearing cache  
* Defining a helper function  

```{r global, message=FALSE}
library(DBI)
library(dplyr)
library(broom)
library(ROCR)
library(extraTrees)
library(lubridate)
library(ggplot2)
library(plotly)

rm(list = ls()) # clear all data
try(invisible(dev.off()),silent=TRUE) # clear all plots

# helper function to print huge dataframe
printWideDataFrame <- function(df, n){
  head(df[c(1:n,(ncol(df)-n):ncol(df))])
}
```


## Fetch from Database
Since the data has already been normalized into Sqlite, a SELECT statement can be used to pull the table into RAM.

In a production environment:  

* For ODBC/JDBC, pass connection credentials to the connection object  
* For REST API "GET", call web service for request/response objects  

```{r get}
# connect to db, fetch table into RAM, disconnect from db
con <- dbConnect(RSQLite::SQLite(), "warehouse.db")
df_orig <- dbGetQuery(con, "select * from sample") %>%
  mutate(
    INSERTED_ON = as.POSIXct(INSERTED_ON),
    MFG_DATE = as.POSIXct(MFG_DATE),
    PASS_FAIL = ifelse(PASS_FAIL==1,0,1))
dbDisconnect(con)

# preview dataframe
printWideDataFrame(df_orig, 15)
```


## Clean Data
Using dplyr commands to:  

* Force response variable to binary  
* Add dummy variables to all strings/factors  
* Remove columns no longer required in calculations  

```{r clean}
# massage for statistics
df_stats <- df_orig %>% 
  fastDummies::dummy_cols() %>%  # add dummy variables for all string columns
  select(-c(ID, INSERTED_ON, MFG_DATE, MAT_VENDOR, PART_VENDOR, SIL_VENDOR, ADHS_VENDOR, SOP_VENDOR))  # drop columns

# preview dataframe
printWideDataFrame(df_stats, 15)
```

## Exploratory Data Analysis
Using javascript wrappers, an html page can be used to show interactive volumes and yields over the given time period.

In a production environment, the UI could be a(n):  

* Web visualization app
* Mobile app
* Other reporting needs for internal customers

```{r eda}
# mfg volume vs time
df_orig %>%
  group_by(MFG_DATE = floor_date(MFG_DATE, "day")) %>%
  summarize(
    QTY_PASS = sum(PASS_FAIL==1),
    QTY_FAIL = sum(PASS_FAIL==0)
    ) %>%
    plot_ly(x = ~MFG_DATE, y = ~QTY_PASS, name = "Pass", type = "bar") %>%
    add_trace(y = ~QTY_FAIL, name = 'Fail') %>%
    layout(
      xaxis=list(title='Manufacturing Date'),
      yaxis=list(title='Volume'),
      barmode="stack",
      title='Semi-Conductor Production Volume vs. Time')

# mfg volume vs time (cumulative)
df_orig %>%
  group_by(MFG_DATE = floor_date(MFG_DATE, "day")) %>%
  summarize(
    QTY = n(),
    QTY_PASS = sum(PASS_FAIL==1),
    QTY_FAIL = sum(PASS_FAIL==0)
  ) %>%
  mutate(
    QTY = cumsum(QTY),
    QTY_PASS = cumsum(QTY_PASS),
    QTY_FAIL = cumsum(QTY_FAIL),
    PERC_PASS = QTY_PASS/QTY
  ) %>%
  plot_ly() %>%
  add_trace(x = ~MFG_DATE, y = ~QTY_PASS, name = "Pass", type = "bar", yaxis = 'y1') %>%
  add_trace(x = ~MFG_DATE, y = ~QTY_FAIL, name = 'Fail', type = "bar", yaxis = 'y1') %>%
  add_trace(x = ~MFG_DATE, y = ~PERC_PASS, type = 'scatter', mode = 'lines', name = 'Yield %', yaxis = 'y2') %>%
  layout(
    xaxis=list(title='Manufacturing Date'),
    yaxis=list(side='left',title='Volume (Cumulative)',showgrid=FALSE,zeroline=FALSE),
    yaxis2=list(side='right',overlaying="y",title='Yield %',showgrid=FALSE,zeroline=FALSE,tickformat="%"),
    barmode="stack",
    title='Semi-Conductor Production Volume vs. Time (Cumulative)'
    )
```

## Model 1: Logistic Regression

"Logistic Regression" represents a generalized linear model commonly used to fit a single binary response.

For more information about this algorithm, see section "ExtraTrees" [here](https://en.wikipedia.org/wiki/Logistic_regression).

Advantages for problem statement:

* Simple approach due to binary response variable

Disadvantages for problem statement:

* Filling NAs with column medians introduces error
* Stepping through to find lowest AIC is computationally expensive


```{r model1}
# create copy
df1 <- df_stats

# impute NAs in dataframe with column medians
for(col in names(df1)) {
  # feature columns only (they start with "F" and the other digits are numeric)
  if((substring(col,1,1) == "F") && !is.na(as.numeric(substring(col,2)))) {
    df1[is.na(df1[,col]), col] <- median(df1[,col], na.rm = TRUE)
  }
}

# preview dataframe
printWideDataFrame(df1, 20)

# initialize models
m1_full = glm(PASS_FAIL ~ ., data=df1, family=binomial(), control = list(maxit = 50))
m1_null = glm(PASS_FAIL ~ 1, data=df1, family=binomial(), control = list(maxit = 50))

# down-select variables
# m1_bwd = step(m1_full, direction="backward"), backward not a good choice for high dimensionality problem
m1_fwd = step(m1_null, scope=list(lower=m1_null, upper=m1_full), direction="forward")
m1_both = step(m1_null, scope = list(upper=m1_full), direction="both")

# compare methods
if(m1_fwd$aic<m1_both$aic){
  print("Step forward selection chosen")
  m1_varModel = m1_fwd
}else{
  print("Step both selection chosen")
  m1_varModel = m1_both
}
m1_formula <- m1_varModel$formula
m1_formula

# This is the result, use this if you need to run this code faster
# m1_formula <- as.formula(
# "PASS_FAIL ~ SIL_VENDOR_eee + F103 + F59 + F21 + F73 + F428 +
# F569 + F64 + F75 + F129 + F433 + F365 + F9 + F443 + F473 +
# F500 + F368 + F488 + SOP_VENDOR_ggg + F411 + F476 + F38 +
# F87 + F104 + F484 + F349 + F84 + F72 + F56 + F554 + F131 +
# F511 + F545 + F470 + F410 + F419 + F418 + F32 + SIL_VENDOR_ccc +
# SOP_VENDOR_aaa + F320 + F66 + F321 + F94 + F132 + F575"
# )

m1_base = glm(m1_formula, data=df1, family=binomial(), control = list(maxit = 50))
summary(m1_base)
```


## Model 2: Extremely Random Trees

"ExtraTrees" represents a Random Forest which:  

* Each tree is trained using the entire sample, rather than bootstrapping
* Top-down splitting from tree learner is randomized

For more information about this algorithm, see section "ExtraTrees" [here](https://en.wikipedia.org/wiki/Random_forest).

Advantages for problem statement:

* Handles NAs gracefully
* Handles noisey data and high dimensionality

Disadvantages for problem statement:

* Difficult to check model integrity due to complexity
* Package documentation brief or missing (in both python and R)


```{r model2}
# create copy
df2 <- df_stats

# preview dataframe
printWideDataFrame(df2, 20)

# run model and summarize
m2_base = extraTrees(df2 %>% select(-PASS_FAIL),df2$PASS_FAIL,numRandomCuts=1,na.action="fuse")
m2_base
# extraTrees does not have a variable importance function
```


## Cross Validation

Perform 5 fold cross validation for both models and generate ROC graphs [here](./docs/ROCs.pdf).


```{r crossval}
# create copy
df <- df_stats

n = 5  # number of folds
df = df[sample(nrow(df)),]  # Randomly shuffle the data
folds = cut(seq(1,nrow(df)),breaks=n,labels=FALSE)  # Create 5 equally size folds

# create empty matrix for accuracy and precision
accuracy = as.data.frame(matrix(data=NA,nrow=n,ncol=2))
precision = as.data.frame(matrix(data=NA,nrow=n,ncol=2))
names(accuracy) = c("m1","m2")
names(precision) = c("m1","m2")
cutoff = 0.50

pdf(file='./docs/ROCs.pdf',width=10,height=7.5)  # begin pdf writer

# Perform 5 fold cross validation
for(i in 1:n){
  # Segment the data by fold using the which() function 
  testIndexes = which(folds==i,arr.ind=TRUE)
  testData = df[testIndexes, ]
  trainData = df[-testIndexes, ]
  
  # model 1: logistic regression  
  m1 = glm(m1_formula,data=trainData,family='binomial',control=list(maxit=50))
  p1 = predict(m1,newdata=testData,type='response')
  pr1 = prediction(p1,testData$PASS_FAIL)
  prf1 = performance(pr1,measure="tpr",x.measure="fpr")
  prec1 = performance(pr1,measure="prec")
  acc1 = performance(pr1,measure="acc")
  auc1 = performance(pr1,measure="auc")
  
  # model 2: extremely random forest
  m2 = extraTrees(trainData %>% select(-PASS_FAIL),trainData$PASS_FAIL,numRandomCuts=1,na.action="fuse")
  p2 = predict(m2,testData %>% select(-PASS_FAIL))
  pr2 = prediction(p2,testData$PASS_FAIL)
  prf2 = performance(pr2,measure="tpr",x.measure="fpr")
  prec2 = performance(pr2,measure="prec")
  acc2 = performance(pr2,measure="acc")
  auc2 = performance(pr2,measure="auc")
  
  # graph results
  par(pty="s")
  plot(prf1,main=paste('ROC: Fold ',i,sep=''),xaxs='i',yaxs='i',asp=1)
  lines(prf2@x.values[[1]],prf2@y.values[[1]],col='red')
  abline(a=0,b=1,lty=2)
  legend('bottomright',
         c(paste('Model 1 | AUC=',format(round(auc1@y.values[[1]],3),3),sep=''),
           paste('Model 2 | AUC=',format(round(auc2@y.values[[1]],3),3),sep='')),
         col=c('black','red'),lty=c(1,1))

  par(pty="m")
  plot(prec1,main=paste('Precision: Fold ',i,sep=''),ylim=c(0.4,1))
  lines(prec2@x.values[[1]],prec2@y.values[[1]],col='red')
  abline(v=0.5,lty=2)
  legend('topleft',c('Model 1','Model 2'),col=c('black','red'),lty=c(1,1))

  plot(acc1,main=paste('Accuracy: Fold ',i,sep=''),ylim=c(0.4,1))
  lines(acc2@x.values[[1]],acc2@y.values[[1]],col='red')
  abline(v=0.5,lty=2)
  legend('topleft',c('Model 1','Model 2'),col=c('black','red'),lty=c(1,1))

  accuracy$m1[i] = acc1@y.values[[1]][max(which(acc1@x.values[[1]]>=cutoff))]
  accuracy$m2[i] = acc2@y.values[[1]][max(which(acc2@x.values[[1]]>=cutoff))]

  precision$m1[i] = prec1@y.values[[1]][max(which(prec1@x.values[[1]]>=cutoff))]
  precision$m2[i] = prec2@y.values[[1]][max(which(prec2@x.values[[1]]>=cutoff))]
  
}

invisible(dev.off())  # close pdf writer
```


## Comparing Models

A simple t-test can be used to compare models on accuracy and precision results.

```{r conclusion}
# defined as null hypothesis: m1-m2=0
accuracy_test = t.test(accuracy$m1,accuracy$m2,conf.level=0.95,paired=T)
precision_test = t.test(precision$m1,precision$m2,conf.level=0.95,paired=T)

accuracy
accuracy_test

if(accuracy_test$p.value>0.05){
  print("Model 1 and Model 2 accuracies are not significantly different.")
}else if(mean(accuracy$m1)>mean(accuracy$m2)){
  print("Model 1 is statistically more accurate than Model 2.")
}else{
  print("Model 2 is statistically more accurate than Model 1.")
}

precision
precision_test

if(precision_test$p.value>0.05){
  print("Model 1 and Model 2 precisions are not significantly different.")
}else if(mean(precision$m1)>mean(precision$m2)){
  print("Model 1 is statistically more precise than Model 2.")
}else{
  print("Model 2 is statistically more precise than Model 1.")
}
```


## Important Variables

The important variables to be exported are from Model 1.  With neither model being significantly different from one another in terms of accuracy or precision, Model 1 variables were chosen due to the lack of documentation for the "ExtraTrees" package on how to extract these variables in Model 2.

The time series graphs of the important variables can be found [here](./docs/ImportantVariables.pdf).

```{r important}
options(warn=-1)
pdf(file='./docs/ImportantVariables.pdf',width=10,height=7.5)  # begin pdf writer
for(name in sort(names(m1_base$coefficients)[-1])){
  p <- df_orig %>%
    fastDummies::dummy_cols() %>%  # add dummy variables for all string columns
    mutate(PASS_FAIL = as.factor(ifelse(PASS_FAIL==1,"PASS","FAIL"))) %>%
    ggplot(aes_string(x = "MFG_DATE", y = name)) +
    geom_point(alpha = 0.4, aes(colour = PASS_FAIL)) +
    scale_color_manual(values = c("FAIL" = "red", "PASS" = "black")) +
    guides(colour = guide_legend(override.aes = list(alpha = 1, size = 2))) +
    ggtitle(paste("Variable:",name)) +
    xlab("Manufacturing Date") +
    ylab("Value")
  print(p)
}
invisible(dev.off())
options(warn=0)
```


## Future Work

* Remove all convergence warnings from models
* Tune existing model parameters
* Research more models for high dimensionality and noisey data with low observations
* Create an enterprise level solution
* Qualitatively investigate all sensors/variables identified a driving failures
* Research for additional relevant data to help classification